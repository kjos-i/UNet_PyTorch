{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMEwMdoSuLspnwtVKjnb0LU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"1xowgZ82RIKp"},"outputs":[],"source":["# INSTALL TORCHMETRICS\n","!pip install torchmetrics"]},{"cell_type":"code","source":["# IMPORT\n","import torch\n","from torch.utils.data import DataLoader, random_split\n","from torch import optim, nn\n","import torch.optim as optim\n","import torch.optim.lr_scheduler as lr_scheduler\n","import torchmetrics\n","from torchmetrics.segmentation import DiceScore\n","\n","from tqdm import tqdm\n","import pandas as pd\n","import os\n","import seaborn as sns\n","sns.set_theme()"],"metadata":{"id":"Si623wJeRRBj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# UPLOAD .py FILES WITH MODEL, DATASET CLASS AND UTILS\n","# UNet_PyTorch_dataset.py, UNet_PyTorch_model.py and UNet_PyTorch_utils.py\n","\n","from google.colab import files\n","uploaded = files.upload()"],"metadata":{"id":"d4OULFYkRWMx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# IMPORT\n","from UNet_PyTorch_dataset import MyDataset\n","from UNet_PyTorch_model import UNet\n","from UNet_PyTorch_utils import val_image_mask, train_loss_iou_dice_acc_graph"],"metadata":{"id":"Z97myMGJRdqk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# CONNECT TO GOOGLE DRIVE\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"ZBOi2M77XQ37"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# LIST DIRECTORY\n","! ls"],"metadata":{"id":"2O1P2N2DTDsc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","This is a modified version of the UNet implementation by found here:\n","https://medium.com/@fernandopalominocobo/mastering-u-net-a-step-by-step-guide-to-segmentation-from-scratch-with-pytorch-6a17c5916114\n","\n","UNet implementation for binary segmentation (background = 0, foreground = 255)\n","Images are resized to 512 x 512 and converted to RGB\n","Masks are resized to 512 x 512 and converted to grayscale\n","\"\"\"\n","\n","# SET VARIABLES\n","ROOT_PATH_DATASET = \"/content/drive/MyDrive/carvana_dataset\" # Path to main dataset folder\n","TRANSFORM = \"transform\" # OR \"transform_augmentation\", but make sure it works on dataset!!\n","LIMIT = 200 # The nr of images included from dataset, to include all set value to None\n","BATCH_SIZE = 4\n","EPOCHS = 100\n","INPUT_CHANNELS = 3 # Number of channels in input images\n","NUM_CLASSES = 1 # For this inplementation the number of classes should be 1\n","LEARNING_RATE = 0.001 # For AdamW optimizer learning rate (LR), PyTorch default 0.001\n","WEIGHT_DECAY = 0.01 # For AdamW optimizer, PyTorch default 0.01\n","LR_S_STEP_SIZE = 30 # For LR scheduler StepLR, nr of epochs before applying gamma decay\n","LR_S_GAMMA = 0.1 # For LR scheduler StepLR (LR * gamma = new LR), PyTorch default 0.1\n","PROBABILITY = 0.5 # Threshold for binary class prediction\n","DICE_INCLUDE_BACKGROUND = True # TorchMetrics dice score calculation, default True\n","ROOT_PATH_SAVE = \"/content/drive/MyDrive\" # OR \"/content\", path where results folder will be created\n","CHECKPOINT_PATH = None # Path to checkpoint file with saved model, optimizer and scheduler parameters\n","\n","\n","# SET DEVICE, RUN ON CUDA IF AVAILABLE\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","elif torch.mps.is_available():\n","    device = torch.device(\"mps\")\n","else:\n","    device = torch.device(\"cpu\")\n","\n","print(\"\\n\")\n","print(\"#\" * 50)\n","print(f\"Running on {device}\")\n","print(\"#\" * 50)\n","print(\"\\n\")\n","\n","\n","# LOAD DATASET\n","ROOT_PATH_DATASET = ROOT_PATH_DATASET\n","TRANSFORM = TRANSFORM\n","LIMIT = LIMIT\n","dataset = MyDataset(root_path=ROOT_PATH_DATASET, transform=TRANSFORM, limit=LIMIT)\n","generator = torch.Generator().manual_seed(55)\n","\n","train_dataset, val_dataset = random_split(dataset, [0.8, 0.2], generator=generator)\n","\n","\n","LEARNING_RATE = LEARNING_RATE\n","BATCH_SIZE = BATCH_SIZE\n","\n","train_dataloader = DataLoader(dataset=train_dataset,\n","                              pin_memory=False,\n","                              batch_size=BATCH_SIZE,\n","                              shuffle=True)\n","val_dataloader = DataLoader(dataset=val_dataset,\n","                              pin_memory=False,\n","                              batch_size=BATCH_SIZE,\n","                              shuffle=True)\n","\n","\n","# SET MODEL COST FUNCTION, OPTIMIZER AND LEARNING RATE SCHEDULER\n","INPUT_CHANNELS = INPUT_CHANNELS\n","NUM_CLASSES = NUM_CLASSES\n","\n","model = UNet(input_channels=INPUT_CHANNELS, num_classes=NUM_CLASSES).to(device)\n","criterion = nn.BCEWithLogitsLoss()\n","optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n","scheduler = lr_scheduler.StepLR(optimizer, step_size=LR_S_STEP_SIZE, gamma=LR_S_GAMMA)\n","\n","if CHECKPOINT_PATH != None:\n","  checkpoint = torch.load(CHECKPOINT_PATH)\n","  model.load_state_dict(checkpoint['model_state_dict'])\n","  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","  scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n","\n","model_name = model.__class__.__name__\n","optimizer_name = optimizer.__class__.__name__\n","criterion_name = criterion.__class__.__name__\n","scheduler_name = scheduler.__class__.__name__"],"metadata":{"id":"kIe4rWorYPNM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# SAVE RESULTS\n","ROOT_PATH_SAVE = ROOT_PATH_SAVE\n","save_directory = \"UNet_training_results\"\n","save_path = os.path.join(ROOT_PATH_SAVE, save_directory)\n","\n","if not os.path.exists(save_path):\n","    os.mkdir(save_path)\n","else:\n","    x = 1\n","    while os.path.exists(save_path):\n","        new_save_directory = (f\"{save_directory}_{x + 1}\")\n","        save_path = os.path.join(ROOT_PATH_SAVE, new_save_directory)\n","        x += 1\n","    os.mkdir(save_path)\n","\n","\n","# TRAIN AND EVALUATE\n","EPOCHS = EPOCHS\n","train_losses = []\n","train_dices = []\n","train_ious = []\n","train_dices = []\n","train_accs = []\n","val_losses = []\n","val_ious = []\n","val_dices = []\n","val_accs = []\n","lr_updates = []\n","\n","for epoch in tqdm(range(EPOCHS), \"EPOCHS\"):\n","    model.train()\n","    train_loss_sum = 0\n","    train_iou_sum = 0\n","    train_dice_sum = 0\n","    train_acc_sum = 0\n","    nr_of_train_loss_items = 0\n","\n","    jaccardindex = torchmetrics.JaccardIndex(task='binary').to(device)\n","    dice_segmentation = DiceScore(num_classes=1, include_background=DICE_INCLUDE_BACKGROUND).to(device)\n","    accuracy = torchmetrics.Accuracy(task='binary').to(device)\n","\n","    print(\"-\" * 50)\n","    print(f\"Beginning of epoch {epoch + 1}\")\n","    print(\"-\" * 50)\n","\n","    for idx, img_mask in enumerate(tqdm(train_dataloader, \"BATCH TRAIN\", position=0, leave=True)):\n","        img = img_mask[0].float().to(device)\n","        mask = img_mask[1].float().to(device)\n","\n","        mask_pred = model(img)\n","        optimizer.zero_grad()\n","\n","        loss = criterion(mask_pred, mask)\n","\n","        prediction = torch.sigmoid(mask_pred)\n","        prediction[prediction < PROBABILITY] = 0\n","        prediction[prediction >= PROBABILITY] = 1\n","\n","        prediction_int = prediction.long()\n","        mask_int = mask.long()\n","\n","        iou = jaccardindex(prediction_int, mask_int)\n","        dice = dice_segmentation(prediction_int, mask_int)\n","        acc = accuracy(prediction_int, mask_int)\n","\n","        train_loss_sum += loss.item()\n","        train_iou_sum += iou.item()\n","        train_dice_sum += dice.item()\n","        train_acc_sum += acc.item()\n","        nr_of_train_loss_items += 1\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","    scheduler.step()\n","    updated_lr = scheduler.get_last_lr()\n","    lr_updates.append(updated_lr)\n","\n","    train_loss = train_loss_sum / len(train_dataloader)\n","    train_iou = train_iou_sum / len(train_dataloader)\n","    train_dice = train_dice_sum / len(train_dataloader)\n","    train_acc = train_acc_sum / len(train_dataloader)\n","\n","    train_losses.append(train_loss)\n","    train_ious.append(train_iou)\n","    train_dices.append(train_dice)\n","    train_accs.append(train_acc)\n","\n","    print(f\"End of training part of epoch {epoch + 1}\")\n","    print(\"-\" * 5)\n","\n","\n","    model.eval()\n","    val_loss_sum = 0\n","    val_iou_sum = 0\n","    val_dice_sum = 0\n","    val_acc_sum = 0\n","    nr_of_val_loss_items = 0\n","    with torch.no_grad():\n","        for idx, img_mask in enumerate(tqdm(val_dataloader, \"BATCH VAL\", position=0, leave=True)):\n","            img = img_mask[0].float().to(device)\n","            mask = img_mask[1].float().to(device)\n","\n","            mask_pred = model(img)\n","            loss = criterion(mask_pred, mask)\n","\n","            prediction = torch.sigmoid(mask_pred)\n","            prediction[prediction < PROBABILITY] = 0\n","            prediction[prediction >= PROBABILITY] = 1\n","\n","            prediction_int = prediction.long()\n","            mask_int = mask.long()\n","\n","            iou = jaccardindex(prediction_int, mask_int)\n","            dice = dice_segmentation(prediction_int, mask_int)\n","            acc = accuracy(prediction_int, mask_int)\n","\n","            val_loss_sum += loss.item()\n","            val_iou_sum += iou.item()\n","            val_dice_sum += dice.item()\n","            val_acc_sum += acc.item()\n","            nr_of_val_loss_items += 1\n","\n","            # SAVE IMAGES FOR SOME OF THE PREDICTIONS\n","            if idx == 0 and epoch > EPOCHS - 5:\n","                val_image_mask(save_path, epoch, idx, img, mask, prediction)\n","\n","        val_loss = val_loss_sum / len(val_dataloader)\n","        val_iou = val_iou_sum / len(val_dataloader)\n","        val_dice = val_dice_sum / len(val_dataloader)\n","        val_acc = val_acc_sum / len(val_dataloader)\n","\n","        val_losses.append(val_loss)\n","        val_ious.append(val_iou)\n","        val_dices.append(val_dice)\n","        val_accs.append(val_acc)\n","\n","        print(f\"End of validation part of epoch {epoch + 1}\")\n","\n","\n","    print(\"-\" * 50)\n","    print(f\"End of epoch {epoch + 1}\")\n","    print(\"-\" * 50)\n","    print(f\"Training Loss EPOCH {epoch + 1}: {train_loss:.4f}\")\n","    print(f\"Training IoU EPOCH {epoch + 1}: {train_iou:.4f}\")\n","    print(\"-\" * 50)\n","    print(f\"Validation Loss EPOCH {epoch + 1}: {val_loss:.4f}\")\n","    print(f\"Validation IoU EPOCH {epoch + 1}: {val_iou:.4f}\")\n","    print(\"-\" * 50)\n","\n","\n","    checkpoint = {'epoch': epoch,\n","                  'model_state_dict': model.state_dict(),\n","                  'optimizer_state_dict': optimizer.state_dict(),\n","                  'scheduler_state_dict': scheduler.state_dict()}\n","    torch.save(checkpoint, (os.path.join(save_path, f\"checkpoint_{epoch + 1}.pth\")))\n","\n","\n","# PANDAS DATAFRAME WITH TRAINING RESULTS\n","epochs_list = list(range(1, EPOCHS + 1))\n","lists = [epochs_list, train_losses, val_losses, train_ious, val_ious,\n","         train_dices, val_dices, train_accs, val_accs, lr_updates]\n","titles = [\"Epoch\", \"Train loss\", \"Val loss\", \"Train IoU\", \"Val IoU\",\n","          \"Train dice\", \"Val dice\", \"Train acc\", \"Val acc\", \"Updated LR\"]\n","training_results_dictionary = {\"Epoch\": epochs_list, \"Train loss\": train_losses,\n","                               \"Val loss\": val_losses, \"Train IoU\": train_ious,\n","                               \"Val IoU\": val_ious, \"Train dice\": train_dices,\n","                               \"Val dice\": val_dices, \"Train acc\": train_accs,\n","                               \"Val acc\": val_accs, \"Updated LR\": lr_updates}\n","df = pd.DataFrame(training_results_dictionary)\n","df.to_csv((os.path.join(save_path, \"training_dataframe.csv\")), index=False)\n","print(\"-\" * 5)\n","print(df)\n","print(\"-\" * 50)\n","\n","\n","# PANDAS DATAFRAME WITH GENERAL INFO\n","info_names = [\"Model\", \"Criterion\", \"Optimizer\", \"Weight decay\", \"LR scheduler\", \"LR start\", \"LR step size\",\n","              \"LR gamma\", \"Input channels\", \"Number of classes\", \"Length dataset\", \"Training images\",\n","              \"Validation images\", \"Limit\", \"Batch size\", \"Epochs\", \"Probability\", \"Dice include background\"]\n","info_items = [model_name, criterion_name, optimizer_name, WEIGHT_DECAY, scheduler_name, LEARNING_RATE,\n","              LR_S_STEP_SIZE, LR_S_GAMMA, INPUT_CHANNELS, NUM_CLASSES, len(dataset), len(train_dataset),\n","              len(val_dataset), LIMIT, BATCH_SIZE, EPOCHS, PROBABILITY, DICE_INCLUDE_BACKGROUND]\n","\n","info_dict = {\"Item\": info_names, \"Info\": info_items}\n","info_df = pd.DataFrame(info_dict)\n","info_df.to_csv((os.path.join(save_path, \"training_info.csv\")), index=False)\n","\n","\n","# GRAPH SHOWING TRAINING AND VALIDATION LOSS AND DICE\n","loss_iou_dice_acc_graph = train_loss_iou_dice_acc_graph(save_path, epochs_list, train_losses, val_losses,\n","                                  train_ious, val_ious, train_dices, val_dices, train_accs, val_accs)\n","\n","\n","torch.cuda.empty_cache()"],"metadata":{"id":"KB_fctirX3z_"},"execution_count":null,"outputs":[]}]}